{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ISL01_Statistical_Learning.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOsOIKEKi+hmFGIHnTx1nE7"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2m3G1GR4f7B"
      },
      "source": [
        "# Linear Regression\n",
        "## Simple Linear Regression\n",
        "**Linear Model**. A linear regression assumes that there is approximately a linear relationship between $X$ and $Y$. In other words, we are *regressing* $Y$ *on* $X$ (or $Y$ *onto* $X$) and write:\n",
        "$$Y \\approx \\beta_0+\\beta_1X, \\qquad (1)$$\n",
        "where $\\beta_0$ and $\\beta_1$ are two unknown constants that represent the *intercept* and *slope*, together the model *coefficients* or *parameters*. Once we have used our training data to produce estimates $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$ for the model coefficients, we can compute:\n",
        "$$\\hat{y} = \\hat{\\beta}_0+\\hat{\\beta}_1x, \\qquad (2)$$\n",
        "where $\\hat{y}$ indicates a prediction of $Y$ on the basis of $X=x$.\n",
        "\n",
        "**Residual Sum of Squares (RSS)**. Let $\\hat{y_i} = \\hat{\\beta}_0+\\hat{\\beta}_1x_i$ be the prediction for $X$ based on th $ith$ value of $X$. Then $e_i = y_i - \\hat{y}_i$ represents the $ith$ *residual* - this is the difference between the $ith$ observed response value and the $ith$ response value that is predicted by our linear model. We define the *residual sum of squares* (RSS) as\n",
        "$$RSS=e_1^2+e_2^2+...+e_n^2,$$\n",
        "or equivalently as\n",
        "$$RSS=(y_1-\\hat{\\beta}_0-\\hat{\\beta}_1x_1)^2+(y_2-\\hat{\\beta}_0-\\hat{\\beta}_1x_2)^2+...+(y_n-\\hat{\\beta}_0-\\hat{\\beta}_1x_n)^2. \\qquad (3)$$\n",
        "\n",
        "**Least Squares**. The least squares approach chooses $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$ to minimize RSS:\n",
        "\n",
        "$$\\hat{\\beta}_1=\\frac{\\sum_{i-1}^n(x_i-\\bar{x}_i)(y_i-\\bar{y}_i)}{\\sum_{i-1}^n(x_i-\\bar{x}_i)^2}, \\quad \\hat{\\beta}_0=\\bar{y}-\\hat{\\beta}_1\\bar{x}, \\qquad (4)$$\n",
        "\n",
        "where $\\bar{y}\\equiv\\frac{1}{n}\\sum_{i-1}^ny_i$ and $\\bar{x}\\equiv\\frac{1}{n}\\sum_{i-1}^nx_i$ are the sample means. In other words, $(4)$ defines the *least squares coefficient estimates* for simple linear regression.\n",
        "\n",
        "### Assesing the Accuaracy of the Coefficient Estimates\n",
        "**Population Regression Line**. If $f$ is to be approximated by a linear function, then we can write this relationship as\n",
        "$$Y = \\beta_0+\\beta_1X+\\epsilon. \\qquad (1)$$\n",
        "Here $\\beta_0$ is the intercept term - that is, the expected value of $Y$ when $X=0$ and $\\beta_1$ is the slope - the average increase in $Y$ associated with a one-unit increase in $X$. The error term is a catch-all for what we miss with this simple model: the true relationship is probably not linear, there may be other variables that cause variation in $Y$, and there may be measurement error. We typically assume that the error term in independent of $X$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLr5_bnu4fqC"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPQQgUtrivAY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}