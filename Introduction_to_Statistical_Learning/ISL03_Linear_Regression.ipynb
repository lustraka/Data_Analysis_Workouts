{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ISL01_Statistical_Learning.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPvGxcvQ1q3+ZfzQ5+c8/gB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lustraka/Data_Analysis_Workouts/blob/main/Introduction_to_Statistical_Learning/ISL03_Linear_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2m3G1GR4f7B"
      },
      "source": [
        "# Linear Regression\n",
        "## Simple Linear Regression\n",
        "**Linear Model**. A linear regression assumes that there is approximately a linear relationship between $X$ and $Y$. In other words, we are *regressing* $Y$ *on* $X$ (or $Y$ *onto* $X$) and write:\n",
        "$$Y \\approx \\beta_0+\\beta_1X, \\qquad (1)$$\n",
        "where $\\beta_0$ and $\\beta_1$ are two unknown constants that represent the *intercept* and *slope*, together the model *coefficients* or *parameters*. \n",
        "\n",
        "**Least Squares Line**. Once we have used our training data to produce estimates $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$ for the model coefficients, we can compute:\n",
        "$$\\hat{y} = \\hat{\\beta}_0+\\hat{\\beta}_1x, \\qquad (2)$$\n",
        "where $\\hat{y}$ indicates a prediction of $Y$ on the basis of $X=x$.\n",
        "\n",
        "**Residual Sum of Squares (RSS)**. Let $\\hat{y_i} = \\hat{\\beta}_0+\\hat{\\beta}_1x_i$ be the prediction for $X$ based on th $ith$ value of $X$. Then $e_i = y_i - \\hat{y}_i$ represents the $ith$ *residual* - this is the difference between the $ith$ observed response value and the $ith$ response value that is predicted by our linear model. We define the *residual sum of squares* (RSS) as\n",
        "$$RSS=e_1^2+e_2^2+...+e_n^2,$$\n",
        "or equivalently as\n",
        "$$RSS=(y_1-\\hat{\\beta}_0-\\hat{\\beta}_1x_1)^2+(y_2-\\hat{\\beta}_0-\\hat{\\beta}_1x_2)^2+...+(y_n-\\hat{\\beta}_0-\\hat{\\beta}_1x_n)^2. \\qquad (3)$$\n",
        "\n",
        "**Least Squares Regression Coefficent Estimates**. The least squares approach chooses $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$ to minimize RSS:\n",
        "\n",
        "$$\\hat{\\beta}_1=\\frac{\\sum_{i-1}^n(x_i-\\bar{x}_i)(y_i-\\bar{y}_i)}{\\sum_{i-1}^n(x_i-\\bar{x}_i)^2}, \\quad \\hat{\\beta}_0=\\bar{y}-\\hat{\\beta}_1\\bar{x}, \\qquad (4)$$\n",
        "\n",
        "where $\\bar{y}\\equiv\\frac{1}{n}\\sum_{i-1}^ny_i$ and $\\bar{x}\\equiv\\frac{1}{n}\\sum_{i-1}^nx_i$ are the sample means. In other words, $(4)$ defines the *least squares coefficient estimates* for simple linear regression.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLr5_bnu4fqC"
      },
      "source": [
        "### Assesing the Accuaracy of the Coefficient Estimates\n",
        "**Population Regression Line**. If $f$ is to be approximated by a linear function, then we can write this relationship as\n",
        "$$Y = \\beta_0+\\beta_1X+\\epsilon. \\qquad (5)$$\n",
        "Here $\\beta_0$ is the intercept term - that is, the expected value of $Y$ when $X=0$ and $\\beta_1$ is the slope - the average increase in $Y$ associated with a one-unit increase in $X$. The error term is a catch-all for what we miss with this simple model: the true relationship is probably not linear, there may be other variables that cause variation in $Y$, and there may be measurement error. We typically assume that the error term in independent of $X$.\n",
        "\n",
        "**Example**.\n",
        "$$Y=2 + 3X + \\epsilon \\qquad (6)$$\n",
        "\n",
        "**Standard Error of $\\hat{\\mu}$**.\n",
        "$$\\text{Var}(\\hat{\\mu})=\\text{SE}(\\hat{\\mu})^2=\\frac{\\sigma^2}{n}, \\qquad (7)$$\n",
        "where $\\sigma$ is the standard deviation of each of the realizations $y_i$ of $Y$ (provided that the $n$ observations are uncorrelated).\n",
        "\n",
        "**Standard Errors associated with $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$**.\n",
        "$$\\text{SE}(\\hat{\\beta}_0)^2=\\sigma^2\\big[\\frac{1}{n}+\\frac{\\bar{x}^2}{\\sum_{i-1}^n(x_i-\\bar{x})^2}\\big], \\quad \\text{SE}(\\hat{\\beta}_1)^2=\\frac{\\sigma^2}{\\sum_{i-1}^n(x_i-\\bar{x})^2}, \\qquad (8)$$\n",
        "\n",
        "$$\\hat{\\beta}_1 \\pm 2 \\cdot \\text{SE}(\\hat{\\beta}_1) \\qquad (9)$$\n",
        "\n",
        "$$\\big[\\hat{\\beta}_1-2\\cdot\\text{SE}(\\hat{\\beta}_1), \\hat{\\beta}_1+2\\cdot\\text{SE}(\\hat{\\beta}_1) \\big] \\qquad (10)$$\n",
        "\n",
        "$$\\hat{\\beta}_0 \\pm 2 \\cdot \\text{SE}(\\hat{\\beta}_0) \\qquad (11)$$\n",
        "\n",
        "**Null Hypothesis**\n",
        "$$H_0 : \\text{There is no relationship between } X \\text{ and } Y. \\qquad (12)$$\n",
        "\n",
        "**Alternative Hypothesis**\n",
        "$$H_1 : \\text{There is some relationship between } X \\text{ and } Y. \\qquad (13)$$\n",
        "\n",
        "**t-statistics**\n",
        "$$t=\\frac{\\hat{\\beta}_1-0}{\\text{SE}(\\hat{\\beta}_1)}, \\qquad (14)$$\n",
        "\n",
        "**p-value**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekR4I3-tLis4"
      },
      "source": [
        "### Assessing the Accuracy of the Model\n",
        "\n",
        "**Residual Standard Error**\n",
        "$$\\text{RSE}=... \\qquad(15)$$\n",
        "$$\\text{RSS}=... \\qquad(16)$$\n",
        "**$R^2$ Statistic**\n",
        "$$R^2=... \\qquad (17)$$\n",
        "$$\\text{Cor}(X,Y)=... \\qquad(18)$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85Mw-2fBLyKY"
      },
      "source": [
        "## Multiple Linear Regression\n",
        "Instead of fitting a separate simple linear regression model for each predictor, a better approach is to extend the simple linear regression model $(5)$ so that it can directly accomodate multiple predictors. We can do this by giving each predictor a separate slope coefficient in a single model. In general, suppose that we have $p$ distinct predictors. Then the multiple linear regression model takes the form\n",
        "$$Y=\\beta_0 +\\beta_1X_1 +\\beta_2X_2 +...+ \\beta_pX_p + \\epsilon, \\qquad (19)$$\n",
        "where $X_j$ represents the $jth$ predictor and $\\beta_j$ quantifies the association between that variable and the response. We interpret $\\beta_j$ as the average effect on $Y$ of a one unit increase in $X_j$, *holding all other predictor fixed*.\n",
        "\n",
        "\n",
        "**Example**\n",
        "$$\\text{Sales}= \\beta_0 + \\beta_{TV}\\times\\text{TV} + \\beta_{Radio}\\times\\text{Radio} + \\beta_{Newpaper}\\times\\text{Newspaper} + \\epsilon\\qquad(20)$$\n",
        "\n",
        "### Estimating the Regression Coefficients\n",
        "As was the case in the simple linear regression setting, the regression coefficients $\\beta_0,\\beta_1,...,\\beta_p$ in $(19)$ are unknown, and must be estimated. Given estimates $\\hat{\\beta_0},\\hat{\\beta_1},...,\\hat{\\beta_p}$, we can make predictions using the formula\n",
        "$$\\hat{y}=\\hat{\\beta_0} + \\hat{\\beta_1}x_1+\\hat{\\beta_2}x_2+...+\\hat{\\beta_p}X_p.\\qquad (21)$$\n",
        "\n",
        "The parameters are estimated using the same least squares approach that we saw in the context of simple linear regression. We choose $\\hat{\\beta_0},\\hat{\\beta_1},...,\\hat{\\beta_p}$ to minimize the sum of squared residals\n",
        "$$\\text{RSS}= \\sum_{i=1}^n(y_i-\\bar{y}_i)^2 = \\sum_{i=1}^n(y_i-\\hat{\\beta}_0 -\\hat{\\beta}_1x_{i1} -\\hat{\\beta}_2x_{i2} - ... -\\hat{\\beta}_px_{ip} )^2\\qquad (22)$$\n",
        "\n",
        "### Some Important Questions\n",
        "#### One: Is There a Relationship Between the Response and Predictors?\n",
        "**F-statistics**. To determine whether there is a relationship between the response and the predictors we test the null hypothesis, \n",
        "$$H_0:\\beta_{TV}=\\beta_{Radio}=\\beta_{Newspaper}=0$$\n",
        "versus the alternative\n",
        "$$H_a:\\text{at least one }\\beta_{j\\in\\{TV, Radio, Newspaper\\}} \\text{ is non-zero}.$$\n",
        "\n",
        "This hypothesis test is performed by computing the $F$-statistic,\n",
        "$$F = \\frac{(\\text{TSS}-\\text{RSS})/p}{\\text{RSS}/(n-p-1)},\\qquad (23)$$\n",
        "where $\\text{TSS}=\\sum(y_i-\\bar{y})^2$ and $\\text{RSS}=\\sum(y_i-\\hat{y}_i)^2$. If the linear model assumptions are correct, on can show that\n",
        "$$E\\{\\text{RSS}/(n-p-1)\\} = \\sigma^2$$\n",
        "and that\n",
        "$$E\\{(\\text{TSS}-\\text{RSS})/p\\} = \\sigma^2.$$\n",
        "Hence, when there is no relationship between the response and predictors, one would expect the $F$-statistics to take on a value close to 1. On the other hand, if $H_a$ is true, then $E\\{(TSS-RSS)/p\\} > \\sigma^2$, so we expect $F$ to be greater than $1$.\n",
        "\n",
        "When $H_0$ is true and the errors $\\epsilon_i$ have a normal distribution, the $F$-statistic follows an $F$-distribution. For any given value of $n$ and $p$, any statistical software package can be used to compute the $p$-value associated with the $F$-statistic using this distribution. Based on this $p$-value, we can determine whether or not to reject $H_0$.\n",
        "\n",
        "**Testing a subset of coefficients**. Sometimes we want to test that a particular subset of $q$ of the coefficients are zero. This corresponds to a null hypothesis\n",
        "$$H_0: \\beta_{p-q+1}=\\beta_{p-q+2}=...=\\beta_p=0,$$\n",
        "where for convenience we have put the variables chosen for omission at the end of the list. In this case we fit a second model that uses the variables *except* those last $q$. Suppose that the residual sum of squares for that model is $\\text{RSS}_0$. then the appropirate F-statistic is\n",
        "\n",
        "$$F=\\frac{(\\text{RSS}_0-\\text{RSS})/q}{\\text{RSS}/(n-p-1)}.\\qquad (24)$$\n",
        "\n",
        "Now we can look at $t$-statistic and corresponding $p$-values of individual predictors. These provide information about whether each individual predictor is related to the response, after adjusting for the other predictors. Each $t$-statistic is the square root of the corresponding $F$-statistic of $F$-test that omits that single variable from the model, leaving all the others in (i.e. $q=1$ in $(24)$). So it reports the *partial effect* of adding that variable to the model.\n",
        "\n",
        "We need to look at the overall $F$-statistic, especially when the number of predictors $p$ is large!\n",
        "\n",
        "#### Two: Deciding on Important Variables\n",
        "\n",
        "#### Three: Model Fit\n",
        "$$\\text{RSE}= \\qquad (25)$$\n",
        "\n",
        "#### Four: Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ZgiP5gOOrDv"
      },
      "source": [
        "## Other Consideration in the Regression Model\n",
        "### Qualitative Predictors\n",
        "**Predictors with Only Two Levels**.\n",
        "$$x_i\n",
        "  \\begin{cases}\n",
        "  1 & \\text{if }ith\\text{ person owns a house} \\\\\n",
        "  0 & \\text{if }ith\\text{ person does not owns a house}\n",
        "  \\end{cases}, \\qquad (26)$$\n",
        "\n",
        "$$y_i= \\qquad (27)$$\n",
        "\n",
        "**Qualitative Predictors with More than Two Levels**.\n",
        "$$x_{i1}=...\\qquad (28)$$\n",
        "$$x_{i2}=...\\qquad (29)$$\n",
        "$$y_i= \\qquad (30)$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4k18EPJmQNrv"
      },
      "source": [
        "### Extensions of the Linear Model\n",
        "#### Removing the Additive Assumption\n",
        "$$Y=...\\qquad (31)$$\n",
        "$$Y=...\\qquad (32)$$\n",
        "\n",
        "**Example**\n",
        "$$\\text{sales}=...\\qquad (33)$$\n",
        "\n",
        "**Example**\n",
        "$$\\text{balance}_i\\approx...\\qquad (34)$$\n",
        "\n",
        "**Example**\n",
        "$$\\text{balance}_i\\approx...\\qquad (35)$$\n",
        "\n",
        "#### Non-linear Relationships\n",
        "**Example**\n",
        "$$\\text{mpg}=...\\qquad (36)$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otDrncosRIog"
      },
      "source": [
        "### Potential Problems\n",
        "1. Non-linearity of the Data\n",
        "2. Correlation of Error Terms\n",
        "3. Non-constant Variance of Error Terms\n",
        "4. Outliers\n",
        "5. High Leverage Points\n",
        "$$h_i=...\\qquad (37)$$\n",
        "6. Collinearity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJbXA1vUR8tf"
      },
      "source": [
        "## Comparison of Linear Regression with $K$-Nearest Neighbors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPQQgUtrivAY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}