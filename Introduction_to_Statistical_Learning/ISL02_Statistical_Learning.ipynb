{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ISL02 Statistical Learning.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMRADv5Id7uEUmRCBNOo2RP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lustraka/Data_Analysis_Workouts/blob/main/Introduction_to_Statistical_Learning/ISL02_Statistical_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRpt8ev1BOtb"
      },
      "source": [
        "# Statistical Learning\n",
        "## What is Statistical Learning?\n",
        "![Key Concepts](http://www.plantuml.com/plantuml/png/RP5FImCn4CNl_HHpR5bAxxAKWWZU51My56IQp6u7ayawcUqVnEzk9Ar5x6M7zzvlzeLabGtL8ekFE4mQkCl64OsW3ULxMAwtQ9_TLxkeSj8qyBezbj6ymQ3asHadgPgb8oLnL2JSf_s9GiL8fkoWZ6tokVgIP7urWnT5J_E7hgjWJ9u2i1XfQJJSS63xTmH0vqP5TuHf5-Z0bPeL39x9ZAMl6taSI7USoKCWLFaDHhaUmQEcJQ2OA_P4lLBEFutJZn75sD1uHr3S8KccMULk0nQgOuTsPiCPnNU4ubEVjEJXihgMQSlB3UrHwGP2wZaREz1B9sT0S7__NQ-kNV1oDbcH-DDhVWC0)\n",
        "\n",
        "Suppose that we observe a quantitative response $Y$ and $p$ different predictors, $X_1, X_2,...,X_p$. We assume that there is some relationship between $Y$ and $X = (X_1, X_2,...,X_p)$, which can be written in the very general form\n",
        "\n",
        "$$Y = f(X) + \\epsilon. \\qquad (1)$$\n",
        "\n",
        "Here $f$ is some fixed but unknown funtion of $X_1, X_2,...,X_p$, and $\\epsilon$ is a random *error term*, which is independent of $X$ and has mean zero. In this formulation, $f$ represents the *systematic* information that $X$ provides about $Y$.\n",
        "\n",
        "### Why Estimate $f$ ?\n",
        "There are two main reasons that we may wish to estimate $f$ : *prediction* and *inference*.\n",
        "\n",
        "**Predicion**. In many situations a set of inputs $X$ are readily available, but the output $Y$ cannot be easily obtained. In this setting, since the error term averages to zero, we can predict $Y$ using\n",
        "\n",
        "$$\\hat{Y} = \\hat{f}(X), \\qquad (2)$$\n",
        "\n",
        "where $\\hat{f}$ represents our estimate of $f$, and $\\hat{Y}$ represents the resulting prediction for $Y$. In this setting, $\\hat{f}$ is often treated as a *black box*, in the sense that one is not typically concerned with the exact form of $\\hat{f}$, provided it yields accurate predictions for $Y$.\n",
        "\n",
        "Consider a given estimate $\\hat{f}$ and a set of predictors $X$, which yields the prediction $\\hat{Y} = \\hat{f}(X)$. Assume for a moment that both $\\hat{f}$ and $X$ are fixed, so that only variability comes from $\\epsilon$. Then, it is easy to show that\n",
        "\n",
        "$$E(Y - \\hat{Y})^2 = E[f(X) + \\epsilon - \\hat{f}(X)]^2 = \\underbrace{[f(X) - \\hat{f}(X)]^2}_\\text{Reducible} + \\underbrace{\\text{Var}(\\epsilon)}_\\text{Irreducible}, \\quad (3)$$\n",
        "\n",
        "where $E(Y - \\hat{Y})^2$ represents the average, or *expected value*, of the squared difference between the predicted and actual value of $Y$, and $\\text{Var}(\\epsilon)$ represents the *variance* associated with the error term $\\epsilon$.\n",
        "\n",
        "> The focus of this repository is on techniques for estimating $f$ with the aim of minimizing the reducible error. It is important to keep in mind that the irreducible error will always provide the upper bound on the accuracz of our prediction on $Y$. This bound is almost always unknown in practice.\n",
        "\n",
        "**Inference**. We are often interested in understanding the association between $Y$ and $X_1, X_2,...,X_p$. In this situation we wish to estimate $f$, but our goal is not necessarily to make predictions for $Y$. Now $\\hat{f}$ cannot be treated as a black box, because we need to know its exact form. In this setting, one may be interested in answering the following questions:\n",
        "- Which predictors are associated with the response?\n",
        "- What is the relationship between the response and each predictor?\n",
        "- Can the relationship between $Y$ and each predictor be adequately summarized using a linear equation, or is the relationship more complicated?\n",
        "\n",
        "> In this repository, we will see a number of examples that fall into the prediction setting, the inference setting, or a combination of the two.\n",
        ">\n",
        "> Depending on whether our ultimate goal is prediction, inference, od a combination of the two, different methods for estimating $f$ may be appropriate."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PV3HPh8DTvfC"
      },
      "source": [
        "### How Do We Estimate $f$ ?\n",
        "**Parametric Methods** involve two-step model-based approach.\n",
        "1. First, we make an assumption about the functional form, or shape, of $f$. For example, one very simple assumption is that $f$ is linear in $X$:\n",
        "$$f(X) = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ... + \\beta_pX_p. \\qquad (4)$$\n",
        "This is a *linear model*. Once we have assumed tha $f$ is linear, the problem of estimating $f$ is greatly simplified. Instead of having to estimate an entirely arbitrary $p$-dimensinal function $f(X)$, one only needs to estimate $p+1$ coefficients $\\beta_0, \\beta_1, ..., \\beta_p$.\n",
        "\n",
        "2. After a model has been selected, we need a procedure that uses the training data to *fit* or *train* the model. In case of the linear model $(4)$, we need to estimate the parameters $\\beta_0, \\beta_1, ..., \\beta_p$. That is, we want to find values of these parameters such that\n",
        "$$Y \\approx \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ... + \\beta_pX_p.$$\n",
        "The most common approach to fitting the model $(4)$ is referred to as *(ordinary) least squares*, but there are also other approaches.\n",
        "\n",
        "**Non-Parametric Methods** do not make explicit assumptions about the functional form of $f$. Instead they seek an estimate of $f$ that gets as close to the data points as possible without being too rough or wiggly. An example is a *thin-plate spline*. In order ro fit a thin-plate spline, the data analyst must select a level of smoothness."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RuLtGT4DX7aB"
      },
      "source": [
        "### The Trade-Off Between Prediction Accuracy and Model Interpretability\n",
        "In general, as the flexibility of a method increases, its interpretability decreases.\n",
        "\n",
        "### Supervised Versus Unsupervised Learning\n",
        "\n",
        "**Supervised Learning**. For each observation of the predictor measurement(s) $x_i, i = 1, ..., n$ there is an associated response measurement $y_i$. We wish to fit a model that relates the response to the predictors, with the aim of accurately predicting the response for future observation (prediction) or better understand the relationship between the response and the predictors (inference).\n",
        "\n",
        "**Unsupervised Learning** describes somewhat more challenging situation in which for every observation $i = 1, ..., n$, we observe a vector of measurements $x_i$ but no associated response $y_i$. One statistical tool that we may use in this setting is *cluster analysis*, or clustering. The goal of cluster analysis is to ascertain, on the basis of $x_1, ..., x_n$, whether the observation fall into relatively distict groups.\n",
        "\n",
        "### Regression Versus Classification Problems"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBM2ZNqKbJVP"
      },
      "source": [
        "## Assessing Model Accuracy\n",
        "\n",
        "![Key Concepts](http://www.plantuml.com/plantuml/png/PL51ReD03Bpp2X-9LlmXRSMfIWu9nqYLinYoKjXDROEQtzS1La8j9sR6CvhHxaMHTGYn7V1HE7WJ8P4G-qj4xqT6_ooSWnFr_E9JDEJWG1ZX_3qYKmirSs8e3-TwLpZxla0PGxGCC2vJ9Bf2Q1XRgbBNNMAP9k8kgDhrx8Rwvq5UBRR9ZYN0K1XGhG5Co06Zf__ADQ10AKknNTRdVaeyR9qcU4uYrOJtqA8kq-3ZZZQp2pupRjrtGgK_ypRJ2hVn4iiimJi-QVcri-P62x4MuXm8dzMHa5zpZnJwspMDNEmfjkkh_080)\n",
        "\n",
        "It is important task to decide for any given set of data which method produces the best results. Selecting the best approach can be one of the most challenging parts of performing statistical learning in practice.\n",
        "\n",
        "### Measuring the Quality of Fit\n",
        "**Training MSE**. In the regression setting, the most commonly-used measure is the *mean squared error* (MSE), given by\n",
        "\n",
        "$$MSE = \\frac{1}{n}\\sum_{i-1}^n(y_i - \\hat{f}(x_i))^2, \\qquad (5)$$\n",
        "\n",
        "where $\\hat{f}(x_i)$ is the prediction that $\\hat{f}$ gives for $ith$ observation.\n",
        "\n",
        "**Test MSE**. We want to know whether $\\hat{f}(x_0) \\approx y_0$, where $(x_0,y_0)$ is a previously unseen test observation not used to train the statistical learning method. If we had a large number of test observation, we could compute\n",
        "\n",
        "$$\\text{Ave}(y_0-\\hat{f}(x_0))^2, \\qquad (6)$$\n",
        "\n",
        "the average squared prediction error for these test observations $(x_0,y_0)$.\n",
        "\n",
        "**Overfitting**. When a given method yields a small training MSE but a large test MSE, we are said to be *overfitting* the data.\n",
        "\n",
        "**Cross-validation**. There is a variety of approaches that can be used to find the flexibility level corresponding to the model with the minimal test MSE. One important method is *cross-validation*, which uses the training data to estimate the test MSE.\n",
        "\n",
        "### Tha Bias-Variance Trade-Off\n",
        "**Expected test MSE**. The *expected test MSE at $x_0$* equals:\n",
        "\n",
        "$$E(y_0 - \\hat{f}(x_0))^2 = \\text{Var}(\\hat{f}(x_0)) + [\\text{Bias}(\\hat{f}(x_0))]^2 + \\text{Var}(\\epsilon), \\qquad (7)$$\n",
        "\n",
        "where $\\text{Var}(\\hat{f}(x_0))$ is the *variance* of $\\hat{f}(x_0)$, $\\text{Bias}(\\hat{f}(x_0))$ is the *bias* of $\\hat{f}(x_0)$, and $\\text{Var}(\\epsilon)$ is the variance of the error terms $\\epsilon$. The overall expected test MSE can be computed by averaging $E(y_0 - \\hat{f}(x_0))^2$ over all possible values of $x_0$ in the test set.\n",
        "- *Variance* $\\text{Var}(\\hat{f}(x_0))$ refers to the amount by which $\\hat{f}$ would change if we estimated it using a different training data set.\n",
        "- *Bias* $\\text{Bias}(\\hat{f}(x_0))$ refers to the error that is introduced by approximating a real-life problem, which may be extremely complicated, by a much simpler model.\n",
        "\n",
        "**Bias-variance trade-off**. As a general rule, as we use more flexible methods, the variance will increase and the bias will decrease. The relative rate of change of these two quantities determines whether the test MSE increases or decreases. The challenge lies in finding a method for which both the variance and the squared bias are low.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8syOcNE2be0e"
      },
      "source": [
        "### The Classification Setting\n",
        "**Training Error Rate**. The most common approach for quantifying the accuracy of our estimate $\\hat{f}$ in the classification setting is the training *error rate*, the proportion of mistakes that are made if we apply our estimate $\\hat{f}$ to the training observations:\n",
        "\n",
        "$$\\frac{1}{n}\\sum_{i=1}^{n}I(y_i\\neq \\hat{y}_i). \\qquad (8)$$\n",
        "\n",
        "Here $\\hat{y}_i$ is the predicted class label for the $ith$ observation using $\\hat{f}$. And $I(y_i\\neq \\hat{y}_i)$ is an *indicator variable* that equals 1 if $y_i\\neq \\hat{y}_i$ and zero if $y_i = \\hat{y}_i$. If $I(y_i = \\hat{y}_i) = 0$ then the $ith$ observation was classified correctly by our classification method; otherwise it was misclassified.\n",
        "\n",
        "**Test Error Rate**. The test error rate associated with a set of test observations of the form $(x_0,y_0)$ is given by\n",
        "\n",
        "$$\\text{Ave}(I(y_0\\neq \\hat{y}_0)), \\qquad (9)$$\n",
        "\n",
        "where $\\hat{y}_0$ is the predicted class label that results from applying the classifier to the test observation with the predictor $x_0$. A *good* classifier is one for which the test error $(9)$ is smallest.\n",
        "\n",
        "**Bayes Classifier**. The test error rate given in $(9)$ is minimized, on average, by a very simple classifier that *assigns each observation to the most likely class, given its predictor values*. In other words, we should simply assign a test observation with predictor vector $x_0$ to the class $j$ for which\n",
        "\n",
        "$$\\text{Pr}(Y-j|X=x_0) \\qquad (10)$$\n",
        "\n",
        "is largest. Note that $(10)$ is a *conditional probability*: it is the probability that $Y=j$, given the observed predictor vector $x_0$.\n",
        "\n",
        "**The Bayes error rate**. Since the Bayes classifier will always choose the class for which ${10}$ is largest, the error rate will be $1 - max_j\\text{Pr}(Y=j|X=x_0)$ at $x_0$. In general, the overall Bayes error rate is given by\n",
        "\n",
        "$$1 - E(max_j\\text{Pr}(Y=j|X)), \\qquad (12)$$\n",
        "\n",
        "where the expectation averages the probatility over all possible values of $X$. The Bayes error rate is analogous to the irreducible error, discussed earlier. \n",
        "\n",
        "> For real data, we do not know the conditional distribution fo $Y|X$, and so computing the Bayes classifier is impossible. Therefore, the Bayes classifier serves as an unattainable gold standard agaist which to compare other methods.\n",
        "\n",
        "**K-Nearest Neighbors**. Given a positive integer $K$ and test observation $x_0$, the *K-Nearest neighbors* (KNN) classifier first identifies the $K$ points in the training data that are closest to $x_0$, represented by $\\mathcal{N}_0$. It then estimates the conditional probability for class $j$ as the fraction of points in $\\mathcal{N}_0$ whose response values equal $j$:\n",
        "\n",
        "$$\\text{Pr}(Y=j|X=x_0) = \\frac{1}{K}\\sum_{i\\in\\mathcal{N}_0}I(y_i=j). \\qquad (12)$$\n",
        "\n",
        "Finally, KNN classifies the test observation $x_0$ to the class with the largest probability from $(12)$.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVNtE_TITuiM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPQQgUtrivAY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHBbuDhb3jzy"
      },
      "source": [
        "---\n",
        "![Expected Value](http://www.plantuml.com/plantuml/png/SoWkIImgoKqioU1orOXKq5M8oKWigOwirOmpKh1LS8rEquZGLD1MY4ajACxCoS-3AKYh1Oh7WjN4bEQbf1Ob5IKcfrQ3bQEhgOsFAKcjAAaEIaqfJSvCoacjLT16qGMH3aiigjM0sQC9q-HPL0JNfgCGKrYQcAAWOQp9vP2Qbm9oDG00)"
      ]
    }
  ]
}